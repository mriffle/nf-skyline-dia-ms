/**
 * # Parameters for nf-maccoss-trex
 *
 * A NextFlow pipeline for analyzing data-ind
 */
params {
    /** \group{Input/Output Parameters} */

    carafe {
        spectra_file = null
        peptide_results_file = null
        carafe_fasta = null
        diann_fasta = null
        cli_options = ''
    }

    // the data to be quantified
    quant_spectra_dir = null      // directory containing the raw data
    quant_spectra_glob = '*.raw'  // which files in this directory to use

    // the data to be used to generate a chromatogram library (e.g. narrow window GPF data)
    chromatogram_library_spectra_dir = null     // directory containing the raw data
    chromatogram_library_spectra_glob = '*.raw' // which files in this directory to use

    spectral_library = null                 // can be blib, dlib, or elib
    fasta = null                            // background FASTA

    files_per_quant_batch = null
    files_per_chrom_lib = null
    random_file_seed = 12

    use_vendor_raw = false

    // placeholders for old skyline parameters
    skyline_document_name = null
    skyline_template_file = null
    skip_skyline = null
    skyline_skyr_file = null

    // Optional PDC study settings
    pdc.client_args = ''
    pdc.study_id = null
    pdc.n_raw_files = null
    pdc.metadata_tsv = null
    pdc.gene_level_data = null
    pdc.s3_download = false

    // The final skyline document will be named using this name. For example,
    // if skyline_custom_name = 'human_dia' then the final Skyline document
    // will be named "human_dia.sky.zip". When importing into PanoramaWeb--this
    // name will appear in the table of imported Skyline documents.
    skyline.document_name = 'final'

    // the search engine to use: must be either 'encyclopedia' or 'diann'
    search_engine = 'encyclopedia'

    // default parameters for Encyclopedia searches, can be overridden by users
    // todo: follow up on these options: -quantifyAcrossSamples true -scoringBreadthType window
    encyclopedia.quant.params           = '-enableAdvancedOptions -v2scoring'
    encyclopedia.chromatogram.params    = '-enableAdvancedOptions -v2scoring'

    // default parameters for DIA-NN
    diann.search_params = '--qvalue 0.01'
    diann.fasta_digest_params = '--cut \'K*,R*,!*P\' --unimod4 --missed-cleavages 1 --min-pep-len 8 --min-pr-charge 2 --max-pep-len 30'

    // whether or not to save the output from encyclopedia running on individual raw/mzML
    // files (e.g. .dia or .features.txt files) to the results directory
    // setting to false can save considerable local disk space if running on AWS Batch
    // the generated chromatogram library (elib) will always be saved, regardless of this setting
    encyclopedia.save_output            = true

    // options for Cascadia (de novo DIA search)
    cascadia.use_gpu = false;       // whether or not to use available GPU, must be set to false if no GPU is available

    // optional user-supplied parameters
    email = null                    // email to notify of workflow outcome, leave null to send no email
    skyline.template_file = null    // the skyline template, if null use default_skyline_template_file
    panorama_upload = false;        // whether or not to upload the results to PanoramaWeb

    msconvert.do_demultiplex = true;          // whether or not to demultiplex with msconvert
    msconvert.do_simasspectra = true;         // whether or not to do simAsSpectra with msconvert
    msconvert.mz_shift_ppm = null             // shift all mz values by n ppn.

    // If set to true, only run msconvert and stop. Resulting mzML files will be saved to the
    // "msconvert" subdirectory of the results directory.
    msconvert_only = false

    // Parameters related to PanoramaWeb
    panorama.domain = 'https://panoramaweb.org'
    panorama.public.key = '7d503a4147133c448c6eaf83bc9b8bc22ace4b7f6d36ca61c9d1ca836c510d10'
    panorama.upload = false             // Whether or not to upload to PanoramaWeb
    panorama.upload_url = null          // The webdav URL of a folder to hold all uploaded files
    panorama.import_skyline = false     // whether or not to import the Skyline into Panorama's internal database

    // replicate metadata
    replicate_metadata = null

    // Skip creating a Skyline document
    skyline.skip = false

    // Set to import a Skyline .skyr file and run any included reports at the end of the workflow
    skyline.skyr_file = null

    // Minimize Skyline document?
    skyline.minimize = false

    skyline.group_by_gene = false
    skyline.protein_parsimony = false
    skyline.fasta = null

    // Whether or not to use hardlinks with Skyline
    skyline.use_hardlinks = false

    qc_report {
        skip = true // Skip QC report generation

        // Normalization and imputation parameters
        exclude_replicates = null // List of replicates to exclude from normalization and batch correction
        exclude_projects = null // List of batches to exclude from normalization and batch correction
        imputation_method = null // Imputation method to use
        normalization_method = 'median' // Normalization method to use for plots in QC report

        standard_proteins = null // List of protein names to plot retention times for
        color_vars = null // List of metadata variables to color PCA plots by
        export_tables = false // Export matrices of normalized precursor and protein quantities?

        // Skyline report templates
        precursor_report_template = 'https://raw.githubusercontent.com/ajmaurais/DIA_QC_report/master/resources/precursor_quality.skyr'
        replicate_report_template = 'https://raw.githubusercontent.com/ajmaurais/DIA_QC_report/master/resources/replicate_quality.skyr'
    }

    batch_report {
        skip = true // Skip batch report generation

        // Metadata keys corresponding to batch levels 1 and 2
        // If null, the project name is used as the batch variable
        batch1 = null
        batch2 = null

        // Metadata key(s) to use as covariates for batch correction
        // Can be either string or list. If null, no covariates are used.
        covariate_vars = null

        // Metadata key indicating replicates which are controls for CV plots
        // If null, all replicates are used in CV distribution plot.
        control_key = null

        // Metadata value(s) mapping to control_key indicating whether a replicate is a control.
        control_values = null

        // File extension for standalone plots. If null, no standalone plots are produced.
        plot_ext = null
    }

    // general workflow params, can be changed
    result_dir = 'results/nf-skyline-dia-ms' /** \type{str} Where results will be saved. */
    report_dir = 'reports/nf-skyline-dia-ms' /** \type{str} Where results will be saved. */

    // use this if no user-supplied skyline template file -- suitable for EncyclopeDIA and DiaNN
    default_skyline_template_file = 'https://github.com/mriffle/nf-skyline-dia-ms/raw/main/resources/template.sky.zip'

    // AWS Batch params
    aws.region = 'us-west-2'
    aws.batch.cliPath = '/usr/local/aws-cli/v2/current/bin/aws'
    aws.batch.logsGroup = '/batch/tei-nextflow-batch'
    aws.batch.maxConnections = 20
    aws.batch.connectionTimeout = 10000
    aws.batch.uploadStorageClass = 'INTELLIGENT_TIERING'
    aws.batch.storageEncryption = 'AES256'
    aws.batch.retryMode = 'standard'

}

plugins {
    id 'nf-amazon'
}

docker {
    enabled = true
}

aws {

    batch {
        // NOTE: this setting is only required if the AWS CLI tool is installed in a custom AMI
        cliPath = params.aws.batch.cliPath
        logsGroup = params.aws.batch.logsGroup
        maxConnections = params.aws.batch.maxConnections
        connectionTimeout = params.aws.batch.connectionTimeout
        uploadStorageClass = params.aws.batch.uploadStorageClass
        storageEncryption = params.aws.batch.storageEncryption
        retryMode = params.aws.batch.retryMode
    }

    region = params.aws.region
}

// Execution Profiles
profiles {

    /*
     * Params for running pipeline on the local computer (e.g.:
     * your laptop). These can be overridden in the local config file.
     */
    standard {
        process.executor = 'local'

        // limit nextflow to running 1 task at a time
        executor.queueSize = 1

        params.max_memory = '12.GB'
        params.max_cpus = 8
        params.max_time = '240.h'

        // where to cache mzml files after running msconvert
        params.mzml_cache_directory = '/data/mass_spec/nextflow/nf-skyline-dia-ms/mzml_cache'
        params.panorama_cache_directory = '/data/mass_spec/nextflow/panorama/raw_cache'
    }

    aws {
        process.executor = 'awsbatch'
        process.queue = 'nextflow_basic_ec2'

        // params for running pipeline on aws batch
        // These can be overridden in local config file

        // max params allowed for your AWS Batch compute environment
        params.max_memory = '250.GB'
        params.max_cpus = 32
        params.max_time = '240.h'

        // where to cache mzml files after running msconvert
        params.mzml_cache_directory = 's3://mc-tei-rex-nextflow-dda/dia/mzml_cache'
        params.panorama_cache_directory = 's3://mc-tei-rex-nextflow-dda/panorama_cache'
    }

    slurm {
        process.executor = 'slurm'

        params.max_memory = '12.GB'
        params.max_cpus = 8
        params.max_time = '240.h'

        // where to cache mzml files after running msconvert
        params.mzml_cache_directory = '/data/mass_spec/nextflow/nf-skyline-dia-ms/mzml_cache'
        params.panorama_cache_directory = '/data/mass_spec/nextflow/panorama/raw_cache'
    }

}

// Manifest
manifest {
    name            = 'nf-skyline-dia-ms'
    author          = 'Michael Riffle'
    homePage        = 'https://github.com/mriffle/nf-skyline-dia-ms'
    description     = 'DIA workflows for TEI-REX project'
    mainScript      = 'main.nf'
    nextflowVersion = '!>=24.04.0'
}

// Capture exit codes from upstream processes when piping
process.shell = ['/bin/bash', '-euo', 'pipefail']
def trace_timestamp = new java.util.Date().format( 'yyyy-MM-dd_HH-mm-ss')
timeline {
    enabled = true
    file    = "${params.report_dir}/execution_timeline_${trace_timestamp}.html"
}
report {
    enabled = true
    file    = "${params.report_dir}/execution_report_${trace_timestamp}.html"
}
trace {
    enabled = true
    file    = "${params.report_dir}/execution_trace_${trace_timestamp}.txt"
}
dag {
    enabled = false
    file    = "${params.report_dir}/pipeline_dag_${trace_timestamp}.html"
}

// Load base.config by default for all pipelines
includeConfig 'conf/base.config'

// Load the images to use for all processes
includeConfig 'container_images.config'

// Load the output file directories
includeConfig 'conf/output_directories.config'
